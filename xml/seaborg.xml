<machine name="seaborg.nersc.gov" nodes="384" mode="default">
<node name="NHII" type="SMP" ncpus="16">
<cpu name="pwr3" speed="375Mhz" inst_per_cyc="2">POWERIII</cpu>
</node>

<metrics>
<metric name="Giga Flop per second" sname="GFlops" units="GFLOP/S" type="double">
 <calc dep_var="hpm(PMAPI),perf">(PM_FPU0_CMPL + PM_FPU1_CMPL + PM_EXEC_FMA)/WTIME</calc>
 <calc dep_var="hpm(PAPI),perf">(PAPI_FP_OPS + PAPI_FML_INS)/WTIME</calc>
</metric>

<metric name="Computational Intensity" sname="Comp_Intens" units="none" type="percent">
 <calc dep_var="hpm(PMAPI),node">(PM_INST_CMPL/(PM_CYC*inst_per_cyc)</calc>
 <calc dep_var="hpm(PAPI),node">(PAPI_TOT_INS/(PAPI_TOT_CYC*inst_per_cyc)</calc>
</metric>

</metrics>

<interconnect name="colony US" type="fat-tree">
<latency_inter>23.4 </latency_inter>
<latency_intra>9.2 </latency_intra>
<too_detailed>
<mpi tasks="16" tasks_per_node="16">
<call name="MPI_Reduce"  bytes="1" time="0.0001"> </call>
<call name="MPI_Reduce"  bytes="2" time="0.0001"> </call>
<call name="MPI_Reduce"  bytes="4" time="0.0004"> </call>
<call name="MPI_Reduce"  bytes="8" time="0.0004"> </call>
<call name="MPI_Reduce"  bytes="16" time="0.0005"> </call>
<call name="MPI_Reduce"  bytes="32" time="0.0005"> </call>
</mpi>
<mpi tasks="32" tasks_per_node="16">
<call name="MPI_Reduce"  bytes="1" time="0.0001"> </call>
<call name="MPI_Reduce"  bytes="2" time="0.0001"> </call>
<call name="MPI_Reduce"  bytes="4" time="0.0004"> </call>
<call name="MPI_Reduce"  bytes="8" time="0.0004"> </call>
<call name="MPI_Reduce"  bytes="16" time="0.0005"> </call>
<call name="MPI_Reduce"  bytes="32" time="0.0005"> </call>
</mpi>
</too_detailed>
</interconnect>
</machine>
